button_element <- remote_driver$findElement(using = 'xpath', value = "/html/body/div/div/div[1]/form/div[2]")
button_element$clickElement()
Sys.sleep(2)
button_element2 <- remote_driver$findElement(using = 'class', value = "mini_card")
button_element2$clickElement()
out <- remote_driver$findElement(using = "xpath", value="/html/body/routable-page/ng-outlet/song-page/div/div/div[2]/div[1]/div/defer-compile[1]/lyrics/div/div/section")
Sys.sleep(2)
lat_long_text <- out$getElementText()
lat_long <- append(lat_long, lat_long_text)
}
#hot100_df_clean <- str_replace_all(hot100_df[3], '"', "")
#hot100_songs <- list(unique(hot100_df[3]))
#hot100_songs_df_2 <- hot100_df[3].strip('"')
#str_replace('"', "", $string);
hot100_songs_3 <- c(hot100_df_unique)
hot100_songs_3
hot100_df_unique
hot100.list <- as.list(as.data.frame(t(hot100_df)))
hot100.list <- as.list(as.data.frame(t(hot100_df_unique)))
hot100.list
hot100_df_unique
hot100.list
hot100list3 <- hot100.list[2]
hot100list3
hot100list3 <- hot100.list[:2]
hot100list3 <- hot100.list[,2]
hot100list3 <- hot100.list[:,2]
hot100list3 <- hot100.list[2,1]
hot100list3 <- hot100.list[2:1]
hot100list3
hot100list3 <- c(hot100.list)
hot100list3
c(hot100_df_unique)
str_replace(c(hot100_df_unique), '"', "")
c(hot100.list)
hot100.list <- as.list(as.data.frame(t(hot100_df_unique)))
hot100_df
hot100_list
hot100_list[3]
hot100_list[1]
as.list(hot100_df)
as.list(hot100_df[1])
as.list(hot100_df[2])
as.list(hot100_df[3])
c(hot100.list)
as.list(hot100_df[3])
as.list(hot100_df_unique[3])
as.list(hot100_df_unique)
test
list.clean(test4)
str_replace(test4, "\\", '')
test4 <- as.list(hot100_df_unique)
str_replace(test4, "\\", '')
str_replace(test4, '"', '')
str_replace_all(test4, '"', '')
str_replace_all(test4, '\\', '')
str_replace_all(test4, "\\", " ")
test5 <- str_replace_all(test4, '"', '')
# Loop through each street name and grab the latitude and longitude coordinates
# for each one. We are basically using the same code as above.
for(i in 1:length(test5)){
remote_driver$navigate("https://genius.com")
remote_driver$refresh()
Sys.sleep(2)
address_element <- remote_driver$findElement(using = 'xpath', value = '/html/body/div/div/div[1]/form/input')
Sys.sleep(1)
address_element$sendKeysToElement(list(test5[i]))
Sys.sleep(2)
button_element <- remote_driver$findElement(using = 'xpath', value = "/html/body/div/div/div[1]/form/div[2]")
button_element$clickElement()
Sys.sleep(2)
button_element2 <- remote_driver$findElement(using = 'class', value = "mini_card")
button_element2$clickElement()
out <- remote_driver$findElement(using = "xpath", value="/html/body/routable-page/ng-outlet/song-page/div/div/div[2]/div[1]/div/defer-compile[1]/lyrics/div/div/section")
Sys.sleep(2)
lat_long_text <- out$getElementText()
lat_long <- append(lat_long, lat_long_text)
}
test4 <- as.list(hot100_df_unique)
test4
str_replace_all(test5, '"\\', "")
test5 <- str_replace_all(test4, '\\"', ',')
convertColsToList(hot100_df_unique, name.list = FALSE, name.vector = FALSE,
factors.as.char = TRUE, as.vector = TRUE)
install.packages("BBmisc")
library(BBmisc)
hot100_list <- convertColsToList(hot100_df_unique, name.list = FALSE, name.vector = FALSE,
factors.as.char = TRUE, as.vector = TRUE)
hot100_list
# Connect to the firefox driver and open it
remote_driver <- driver[["client"]]
remote_driver$open()
# Initialize some empty lists
lat_long <- c()
out <- c()
test <- c("Perfect", "One Dance", "Havana")
# Loop through each street name and grab the latitude and longitude coordinates
# for each one. We are basically using the same code as above.
for(i in 1:length(hot100_list)){
remote_driver$navigate("https://genius.com")
remote_driver$refresh()
Sys.sleep(2)
address_element <- remote_driver$findElement(using = 'xpath', value = '/html/body/div/div/div[1]/form/input')
Sys.sleep(1)
address_element$sendKeysToElement(list(hot100_list[i]))
Sys.sleep(2)
button_element <- remote_driver$findElement(using = 'xpath', value = "/html/body/div/div/div[1]/form/div[2]")
button_element$clickElement()
Sys.sleep(2)
button_element2 <- remote_driver$findElement(using = 'class', value = "mini_card")
button_element2$clickElement()
out <- remote_driver$findElement(using = "xpath", value="/html/body/routable-page/ng-outlet/song-page/div/div/div[2]/div[1]/div/defer-compile[1]/lyrics/div/div/section")
Sys.sleep(2)
lat_long_text <- out$getElementText()
lat_long <- append(lat_long, lat_long_text)
}
# Read the page for Billboard number-one single "Hot 100 Era"
billboard_singles <- read_html("https://en.wikipedia.org/wiki/List_of_Billboard_number-one_singles")
# Get list of Hot 100 Era Years
hot100_years_full <- billboard_singles %>%
html_nodes("tbody") %>%
html_nodes("tr") %>%
html_nodes("td") %>% # :nth-child(11)
html_nodes("a") %>%
html_text()
# Trimming to years of Hot 100 1958-2019
hot100_years <- hot100_years_full[c(58:62)] #(1:62) #trimming the list of years
# Choose a specific year to look at
hot100_years <- "2018"
# Initialize an empty list
hot100_list <- c()
#----- Iterating through the list of years to get all of the Hot 100 Era Tables
for(i in hot100_years){
hot100_session <- html_session("https://en.wikipedia.org/wiki/List_of_Billboard_number-one_singles") # Initializing HTML Session
hot100_link <- hot100_session %>%
follow_link(i)
# Get the charts list for that specific year
hot100_chart <- hot100_link %>%
html_nodes("table.wikitable.plainrowheaders") %>%
html_table(fill = TRUE, header = 1)
# Append the next year of songs to the full list
hot100_list <- append(hot100_list, hot100_chart)
#hot100_list <-
#Sys.sleep(5)
}
# Convert the list to data frame
hot100_df <- data.frame(hot100_list)
h
hot100_df_unique <- unique(hot100_df[3]) #Get only unique songs from the dataframe
hot100_df_unique
# Specify the List of Songs to get Lyrics for
song_list <- c("Perfect", "Havana", "God's Plan", "Nice for What", "This Is America", "Psycho", "Sad!",
"I Like It", "In My Feelings", "Girls Like You", "Thank U, Next", "Sicko Mode")
# Loop through each street name and grab the latitude and longitude coordinates
# for each one. We are basically using the same code as above.
for(i in 1:length(song_list)){
remote_driver$navigate("https://genius.com")
remote_driver$refresh()
Sys.sleep(2)
address_element <- remote_driver$findElement(using = 'xpath', value = '/html/body/div/div/div[1]/form/input')
Sys.sleep(1)
address_element$sendKeysToElement(list(song_list[i]))
Sys.sleep(2)
button_element <- remote_driver$findElement(using = 'xpath', value = "/html/body/div/div/div[1]/form/div[2]")
button_element$clickElement()
Sys.sleep(2)
button_element2 <- remote_driver$findElement(using = 'class', value = "mini_card")
button_element2$clickElement()
out <- remote_driver$findElement(using = "xpath", value="/html/body/routable-page/ng-outlet/song-page/div/div/div[2]/div[1]/div/defer-compile[1]/lyrics/div/div/section")
Sys.sleep(2)
lat_long_text <- out$getElementText()
lat_long <- append(lat_long, lat_long_text)
}
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$open()
remote_driver <- driver[["client"]]
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$open()
driver1 <- rsDriver(browser = c("firefox"))
remote_driver <- driver1[["client"]]
remote_driver$open()
# Initialize empty lists
lat_long <- c()
out <- c()
# Specify the List of Songs to get Lyrics for
song_list <- c("Perfect", "Havana", "God's Plan", "Nice for What", "This Is America", "Psycho", "Sad!",
"I Like It", "In My Feelings", "Girls Like You", "Thank U, Next", "Sicko Mode")
# Loop through each street name and grab the latitude and longitude coordinates
# for each one. We are basically using the same code as above.
for(i in 1:length(song_list)){
remote_driver$navigate("https://genius.com")
remote_driver$refresh()
Sys.sleep(2)
address_element <- remote_driver$findElement(using = 'xpath', value = '/html/body/div/div/div[1]/form/input')
Sys.sleep(1)
address_element$sendKeysToElement(list(song_list[i]))
Sys.sleep(2)
button_element <- remote_driver$findElement(using = 'xpath', value = "/html/body/div/div/div[1]/form/div[2]")
button_element$clickElement()
Sys.sleep(2)
button_element2 <- remote_driver$findElement(using = 'class', value = "mini_card")
button_element2$clickElement()
out <- remote_driver$findElement(using = "xpath", value="/html/body/routable-page/ng-outlet/song-page/div/div/div[2]/div[1]/div/defer-compile[1]/lyrics/div/div/section")
Sys.sleep(2)
lat_long_text <- out$getElementText()
lat_long <- append(lat_long, lat_long_text)
}
lat_long
# Loop through each street name and grab the latitude and longitude coordinates
# for each one. We are basically using the same code as above.
for(i in 1:length(song_list)){
remote_driver$navigate("https://genius.com")
remote_driver$refresh()
Sys.sleep(2)
address_element <- remote_driver$findElement(using = 'xpath', value = '/html/body/div/div/div[1]/form/input')
Sys.sleep(1)
address_element$sendKeysToElement(list(song_list[i]))
Sys.sleep(2)
button_element <- remote_driver$findElement(using = 'xpath', value = "/html/body/div/div/div[1]/form/div[2]")
button_element$clickElement()
Sys.sleep(2)
button_element2 <- remote_driver$findElement(using = 'class', value = "mini_card")
button_element2$clickElement()
Sys.sleep(2)
out <- remote_driver$findElement(using = "xpath", value="/html/body/routable-page/ng-outlet/song-page/div/div/div[2]/div[1]/div/defer-compile[1]/lyrics/div/div/section")
Sys.sleep(2)
lat_long_text <- out$getElementText()
lat_long <- append(lat_long, lat_long_text)
}
lat_long
lat_long <- lat_long[(-c(1)),]
lat_long1 <- lat_long[-c(1)]
lat_long1
lat_long <- lat_long[-c(1)]
write.csv(lat_long, file = "test2018lyrics")
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
library("wordcloud")
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
library("wordcloud")
text <- readLines(lyrics_list)
text <- read.csv(file = '/Users/jacobmannix/Desktop/test2018lyrics')
docs <- Corpus(VectorSource(text))
# Load
library("tm")
docs <- Corpus(VectorSource(text))
inspect(docs)
# Cleaning up the docs
docs <- tm_map(docs, toSpace, "/")
# Cleaning up the docs
docs <- tm_map(docs, " ", "/")
docs <- tm_map(docs, " ", "@")
docs <- tm_map(docs, " ", "\\|")
docs <- tm_map(docs, content_transformer(tolower))
docs
# Cleaning up the docs
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
text
text <- readLines(lat_long)
text <- readLines(lat_long)
text <- read.csv(file = '/Users/jacobmannix/Desktop/test2018lyrics')
View(text)
docs <- Corpus(VectorSource(lat_long))
inspect(docs)
# Cleaning up the docs
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
head(d, 15)
# Creating a Word Cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
#exploring frequent terms and there associations
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
head(d, 10)
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
findAssocs(dtm, terms = "psycho", corlimit = 0.3)
head(d, 10)
# Plotting word frequencies
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
col ="lightblue", main ="Most frequent words",
ylab = "Word frequencies")
# SENTIMENT ANALYSIS
install.packages('sentimentr')
# SENTIMENT ANALYSIS
#install.packages('sentimentr')
library(sentimentr)
sentiment_by(docs, by = NULL)
sentiment_by(lat_long, by = NULL)
sentiment_by(lat_long)
sentiment(lat_long)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
driver <- rsDriver(browser = c("firefox"))
library(RSelenium)
driver <- rsDriver(browser = c("firefox"))
driver$server$stop()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
load("/Users/jacobmannix/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Web Scrapping/Assignment/Web Scraping Assignment Data Variables/docs1980_2015.RData") # docs
for (i in docyear){
dtm <- TermDocumentMatrix(i)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
headd <- head(d, 15)
docshead <- append(docshead, headd)
}
docyear <- c(docs1980, docs1985, docs1990, docs1995, docs2000, docs2005, docs2010, docs2015)
for (i in docyear){
dtm <- TermDocumentMatrix(i)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
headd <- head(d, 15)
docshead <- append(docshead, headd)
}
library(rvest)
library(RSelenium)
library(tidyverse)
library(stringr)
library("tm") # text mining
for (i in docyear){
dtm <- TermDocumentMatrix(i)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
headd <- head(d, 15)
docshead <- append(docshead, headd)
}
m <- as.matrix(dtm)
for (i in docyear){
dtm <- TermDocumentMatrix(i)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
headd <- head(d, 15)
docshead <- append(docshead, headd)
}
# Creating a Term Document Matrix to display most frequently used words
dtm <- TermDocumentMatrix(docs1980)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v)) # ,freq=v
headd <- head(d, 15)
docshead <- append(docshead, headd)
# Creating a Term Document Matrix to display most frequently used words
dtm <- TermDocumentMatrix(docs1985)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v)) # ,freq=v
headd <- head(d, 15)
docshead <- append(docshead, headd)
# Creating a Term Document Matrix to display most frequently used words
dtm <- TermDocumentMatrix(docs1990)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v)) # ,freq=v
headd <- head(d, 15)
docshead <- append(docshead, headd)
# Creating a Term Document Matrix to display most frequently used words
dtm <- TermDocumentMatrix(docs1995)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v)) # ,freq=v
headd <- head(d, 15)
docshead <- c()
for (i in docyear){
dtm <- TermDocumentMatrix(i)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
headd <- head(d, 15)
docshead <- append(docshead, headd)
}
# Creating a Term Document Matrix to display most frequently used words
dtm <- TermDocumentMatrix(docs1980)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v)) # ,freq=v
headd <- head(d, 15)
docshead <- append(docshead, headd)
docshead
headd
View(headd)
v
head(v, 10)
dtm <- TermDocumentMatrix(docyear)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v)) # ,freq=v
headd <- head(d, 20)
docs <- Corpus(VectorSource(docyear)) # lyrics_list
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Cleaning up the docs further
docs <- tm_map(docs, content_transformer(tolower)) #to lower case
docs <- tm_map(docs, removeNumbers) # Remove numbers
docs <- tm_map(docs, removeWords, stopwords("english")) # Remove english common stopwords
docs <- tm_map(docs, removePunctuation) # Remove punctuations
docs <- tm_map(docs, stripWhitespace) # Eliminate extra white spaces
docs <- tm_map(docs, removeWords, c("chorus", "verse"))
# docs <- tm_map(docs, removeWords, c("niggas", "fuckin", "bitch")) # Remove your own stop word
# Creating a Term Document Matrix to display most frequently used words
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v)) # ,freq=v
head(d, 20)
d <- data.frame(word = names(v,freq=v)) #
head(d, 20)
d <- data.frame(word = names(v),freq=v) #
head(d, 20)
docsALL <- Corpus(VectorSource(docyear)) # lyrics_list
# text <- read.csv(file = '/Users/jacobmannix/Desktop/test1980lyrics.csv')
docyearALL <- c(docs1980, docs1985, docs1990, docs1995, docs2000, docs2005, docs2010, docs2015)
docsAll <- Corpus(VectorSource(docyearALL)) # lyrics_list
docs <- Corpus(VectorSource(docyearALL)) # lyrics_list
# Cleaning up the docs
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Cleaning up the docs further
docs <- tm_map(docs, content_transformer(tolower)) #to lower case
docs <- tm_map(docs, removeNumbers) # Remove numbers
docs <- tm_map(docs, removeWords, stopwords("english")) # Remove english common stopwords
docs <- tm_map(docs, removePunctuation) # Remove punctuations
docs <- tm_map(docs, stripWhitespace) # Eliminate extra white spaces
docs <- tm_map(docs, removeWords, c("chorus", "verse"))
# docs <- tm_map(docs, removeWords, c("niggas", "fuckin", "bitch")) # Remove your own stop word
docsAll <- docs
save.image("~/Desktop/docsAll.RData")
head(d, 20) # Top 20 words and frequency from all specified years
dtm <- TermDocumentMatrix(docsAll)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 20) # Top 20 words and frequency from all specified years
kable(head(d, 20), format = 'markdown') # Top 20 words and frequency from all specified years
load("/Users/jacobmannix/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Web Scrapping/Assignment/Web Scraping Assignment Data Variables/Songs1980_2015.RData") # Songs
load("/Users/jacobmannix/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Web Scrapping/Assignment/Web Scraping Assignment Data Variables/lyrics1980_2015.RData") # Lyrics
load("/Users/jacobmannix/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Web Scrapping/Assignment/Web Scraping Assignment Data Variables/docs1980_2015.RData") # docs
<center><img src="/Users/jacobmannix/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Web Scrapping/Assignment/WordClouds/AllWordClouds.png" alt="drawing"/></center>
knit_with_parameters('~/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Web Scrapping/Assignment/Web Scraping Assignment_edit.Rmd')
shiny::runApp('~/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Shiny/Shiny Assignment/R_Shiny_Assignment')
runApp('~/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Shiny/Shiny Assignment/R_Shiny_Assignment')
runApp('~/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Shiny/Shiny Assignment/R_Shiny_Assignment')
# Libraries
library(shiny)
library(shinythemes)
runApp('~/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Shiny/Shiny Assignment/R_Shiny_Assignment')
output$table2 <- renderTable({
xtable(fifa_subset_att())
# Libraries
library(shiny)
shiny::runApp('~/Box Sync/M.S. Analytics/Analytics Fall/DATA 900/Shiny/Shiny Assignment/R_Shiny_Assignment')
